{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active RL with Q-learning\n",
    "\n",
    "We're looking at the case of Reinforcement Learning with partially observable rewards.\n",
    "\n",
    "The agent can query the reward function and there is an (e.g. fixed) cost of every query the agent makes.\n",
    "\n",
    "The agent knows the dynamics of the query operation, and can make decisions based on it.\n",
    "\n",
    "There should be pricipled approaches to making these query decisions, possibly within the standard RL framework (e.g. based on planning incorporating the possible outcomes of the query), but we focus on heuristics for now.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------\n",
    "Now I'll go through the experiment script that I've been using.  It is self contained, besides a few imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import scipy\n",
    "# TODO: fix this import\n",
    "#import scipy.stats\n",
    "import numpy\n",
    "np = numpy\n",
    "from collections import OrderedDict\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use the global variables throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the current time-step, state, and action\n",
    "step = 0\n",
    "current_state = 0\n",
    "current_action = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gridworld Environment\n",
    "\n",
    "We ues an 8x8 gridworld for our experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# states (lexical order)\n",
    "grid_width = 8 \n",
    "states = range(grid_width**2)\n",
    "# actions: stay, N, E, S, W\n",
    "actions = range(5)\n",
    "\n",
    "\n",
    "# The rewards are sparse and stochastic (Bernoulli, with p drawn from U(0,1)).\n",
    "prob_zero_reward = .9\n",
    "reward_probabilities = np.random.binomial(1, 1 - prob_zero_reward, len(states)) * np.random.uniform(0, 1, len(states))\n",
    "\n",
    "\n",
    "# The agent seeks to maximize reward over a life-time of 100000 time-steps.\n",
    "num_steps = 100000\n",
    "\n",
    "# There is some chance that the agent moves randomly, instead of as the action it chose dictates.\n",
    "prob_random_move = 0.1 \n",
    "# There are also poisson random resets bringing the agent back to the start state.\n",
    "prob_random_reset = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In practice, we use the same MDP for most experiments, so that we can do fair comparisons of different methods.\n",
    "#reward_probabilities = np.load('/u/kruegerd/CS188.1x-Project3/fixed_mdp0.npy')\n",
    "# Or we use a suite of random MDPs, in case the one above is biased against certain methods.\n",
    "#reward_probabilities = np.load('/u/kruegerd/CS188.1x-Project3/200mdps.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# basic environment dynamics.\n",
    "# running into walls leaves you where you are.\n",
    "def get_next_state(state, action):\n",
    "    row, column = state / grid_width, state % grid_width\n",
    "    if np.random.binomial(1, prob_random_move): # action is replaced at random\n",
    "        action = np.argmax(np.random.multinomial(1, [.2,.2,.2,.2,.2]))\n",
    "    if action == 1 and row > 0:\n",
    "        next_state = state - grid_width\n",
    "    elif action == 2 and column < grid_width - 1:\n",
    "        next_state = state + 1\n",
    "    elif action == 3 and row < grid_width - 1:\n",
    "        next_state = state + grid_width\n",
    "    elif action == 4 and column > 0:\n",
    "        next_state = state - 1\n",
    "    else:\n",
    "        next_state = state\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "We learn via Q-learning, keeping a table of Q-values.\n",
    "Since we only get reward when we query it, we need a default value to replace the observed reward with, when we don't query.  Our solution is to maintain a beta distribution over the parameters of the reward function for every (s,a) pair and take the expectation of this distribution, although sampling could also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_values = [[0,0,0,0,0] for state in states]\n",
    "\n",
    "# for now, we use epsilon greedy\n",
    "epsilon = 0.1\n",
    "gamma = .9 # discount factor\n",
    "learning_rate = .1\n",
    "\n",
    "def update_q(state0, action, state1, reward, query):\n",
    "    if not query:\n",
    "        reward = expected_reward(state0, action)\n",
    "    old = Q_values[state0][action]\n",
    "    new = reward + gamma*np.max(Q_values[state1])\n",
    "    Q_values[state0][action] = (1-learning_rate)*old + learning_rate*new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian updating\n",
    "\n",
    "To compute the expected reward, we need to keep track of our reward observations and queries\n",
    "We can also use this information to maintain a distribution over states, which could allow us to prefer learning about commonly visited states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# track the number of times each (s,a) has been visited/queried, and the amount of rewards observed. \n",
    "num_visits = [[0,0,0,0,0] for state in states]\n",
    "num_queries = [[0,0,0,0,0] for state in states]\n",
    "observed_rewards = [[0,0,0,0,0] for state in states] \n",
    "\n",
    "\n",
    "# We learn a beta-bernoulli distribution over the reward function of each (s,a)\n",
    "# we use a lookup table to speed up computation\n",
    "beta_entropy_lookup = {}\n",
    "\n",
    "def reward_entropy(state, action):\n",
    "    # alpha_, beta_ are the empirical counts\n",
    "    alpha_ = total_r_observed[state][action]\n",
    "    beta_ = nqueries[state][action] - alpha_\n",
    "    if (alpha_, beta_) in beta_entropy_lookup:\n",
    "        entry = beta_entropy_lookup[(alpha_, beta_)]\n",
    "    else:\n",
    "        # we use Jeffrey's prior (adding .5 to get pseudo-counts)\n",
    "        entry = scipy.stats.beta(alpha_ + .5, beta_ + .5).entropy()\n",
    "        beta_entropy_lookup[(alpha_, beta_)] = entry\n",
    "    return entry\n",
    "\n",
    "def expected_reward(state, action):\n",
    "    alpha = total_r_observed[state][action] + .5\n",
    "    beta = ( nqueries[state][action] + 1) - alpha\n",
    "    return alpha / (alpha + beta)\n",
    "\n",
    "\n",
    "# learn a distribution over states (dirichlet-multinomial) \n",
    "def state_entropy():\n",
    "    return scipy.stats.dirichlet([sum(nvisit) + .5 for nvisit in nvisits]).entropy()\n",
    "def state_probability(state):\n",
    "    (sum(nvisits[state]) + .5) / (step + .5*len(states))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning and Query Functions\n",
    "We use simple heuristics to decide when to query the reward.\n",
    "A more principled approach should work better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_cost = 1.\n",
    "\n",
    "# Query functions\n",
    "query_fns = OrderedDict()\n",
    "\n",
    "# simple baselines\n",
    "query_fns['every time'] = lambda : True\n",
    "\n",
    "budget = 'low'\n",
    "if budget == 'high':\n",
    "    max_num_queries = 100000. / query_cost\n",
    "elif budget == 'med':\n",
    "    max_num_queries = 30000. / query_cost\n",
    "elif budget == 'low':\n",
    "    max_num_queries = 10000. / query_cost\n",
    "elif budget == 'very_low':\n",
    "    max_num_queries = 1000. / query_cost\n",
    "\n",
    "query_fns['first N steps'] = lambda : step < max_num_queries\n",
    "# in practice these end up making ~90% and 50% as many queries as the above:\n",
    "query_fns['first N state visits'] = lambda : sum(num_visits[current_state]) < (max_num_queries / len(states))\n",
    "query_fns['first N (state,action) visits'] = lambda : num_visits[current_state][action] < (max_num_queries / (len(states) * len(actions)))\n",
    "\n",
    "query_probability_decay = 1 - 1. / max_num_queries \n",
    "query_fns['decaying probability'] = lambda : np.random.binomial(1, query_probability_decay**step)\n",
    "\n",
    "#---------------------------------\n",
    "# Some more complicated heuristics\n",
    "\n",
    "# query based on entropy of P(r|s,a):\n",
    "query_fns['r entropy threshold'] = lambda : reward_entropy(current_state, current_action) > -1\n",
    "query_fns['stochastic r entropy'] = lambda : np.random.binomial(1, np.exp(reward_entropy(current_state, current_action)))\n",
    "\n",
    "# query based on number of state visits:\n",
    "query_fns['prob = proportion of state visits'] = lambda : np.random.binomial(1, (sum(nvisits[current_state]) + 50) / (sum(nvisits) + 50. * len(states)))\n",
    "query_fns['prob = proportion of state-action visits'] = lambda : np.random.binomial(1, (sum(nvisits[current_state][action]) + 10) / (sum(nvisits) + 10. * len(states) * len(actions)))\n",
    "\n",
    "\n",
    "# query based on entropy of softmax policy (N.B. we don't actually use that policy):\n",
    "def multinomial_entropy(probs): # TODO: stability?\n",
    "    return -np.sum(probs * np.log(probs))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def softmax_entropy(state, action):\n",
    "    return multinomial_entropy(softmax(Q_values[state]))\n",
    "\n",
    "query_fns['softmax entropy threshold'] = lambda : softmax_entropy(current_state, current_action) > 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "    current_action = np.argmax(Q_values[current_state])\n",
    "    if np.random.binomial(1, epsilon): # take a random action\n",
    "        current_action = np.argmax(np.random.multinomial(1, [.2, .2, .2, .2, .2]))\n",
    "    nvisits[current_state][current_action] += 1\n",
    "    reward = np.random.binomial(1, reward_probabilities[current_state])\n",
    "    total_reward += reward \n",
    "    query = query_fn()\n",
    "    if query:\n",
    "        nqueries[current_state][current_action] += 1\n",
    "        observed_rewards[current_state][current_action] += reward\n",
    "\n",
    "    old_state = current_state\n",
    "    if np.random.binomial(1, prob_random_reset): # reset to initial state\n",
    "        current_state = 0\n",
    "\n",
    "    # perform q-learning \n",
    "    update_q(old_state, current_action, current_state, reward, query)\n",
    "\n",
    "total_observed_rewards = sum([ sum(r_observed) for r_observed in observed_rewards])\n",
    "total_nqueries = sum([ sum(nqueries_s) for nqueries_s in nqueries])\n",
    "total_query_cost = query_cost * total_nqueries\n",
    "performance = total_reward - total_query_cost\n",
    "\n",
    "print \"total_reward =\", total_reward\n",
    "print \"total_observed_reward =\", total_observed_rewards\n",
    "print \"total_nqueries =\", total_nqueries\n",
    "print \"performance =\", performance\n",
    "performances.append(performance)\n",
    "print time.time() - t1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
